services:
  ap:
    build:
      context: .
      network: host
      args:
        # proxy for build stage
        http_proxy: "http://127.0.0.1:3128"
        https_proxy: "http://127.0.0.1:3128"
        no_proxy: "${no_proxy}"

        # pin torch nightly version (exact)
        TORCH_VERSION: "2.8.0.dev20250506+cpu"
        TORCH_INDEX_URL: "https://download.pytorch.org/whl/nightly/cpu"

        # autoparallel
        AUTOPARALLEL_REPO: "https://github.com/meta-pytorch/autoparallel.git"
        AUTOPARALLEL_REF: "main"

    image: ap-llama3-cpu:torch-2.8.0.dev20250506
    network_mode: host
    working_dir: /workspace
    shm_size: "8g"

    environment:
      # proxy for runtime too
      http_proxy: "http://127.0.0.1:3128"
      https_proxy: "http://127.0.0.1:3128"
      no_proxy: "${no_proxy}"

      HF_HOME: /cache/huggingface
      TRANSFORMERS_CACHE: /cache/huggingface/transformers
      HF_HUB_DISABLE_TELEMETRY: "1"
      TOKENIZERS_PARALLELISM: "false"
      PYTHONUNBUFFERED: "1"

    volumes:
      - "${PWD}:/workspace:rw"
      - "${HOME}/models:/models:ro"
      - "${HOME}/.cache/ap_hf:/cache:rw"

    command: ["/bin/bash"]
